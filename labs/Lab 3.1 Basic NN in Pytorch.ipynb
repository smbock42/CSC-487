{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 3.1404871940612793\n",
      "epoch 1: loss is 2.3001060485839844\n",
      "epoch 2: loss is 1.5649179220199585\n",
      "epoch 3: loss is 1.0467150211334229\n",
      "epoch 4: loss is 0.7963911294937134\n",
      "epoch 5: loss is 0.6812124848365784\n",
      "epoch 6: loss is 0.6105504035949707\n",
      "epoch 7: loss is 0.5587332844734192\n",
      "epoch 8: loss is 0.5171430110931396\n",
      "epoch 9: loss is 0.4821283519268036\n",
      "epoch 10: loss is 0.45186424255371094\n",
      "epoch 11: loss is 0.4253222346305847\n",
      "epoch 12: loss is 0.40186363458633423\n",
      "epoch 13: loss is 0.38104790449142456\n",
      "epoch 14: loss is 0.362533837556839\n",
      "epoch 15: loss is 0.34603509306907654\n",
      "epoch 16: loss is 0.33130520582199097\n",
      "epoch 17: loss is 0.3181341886520386\n",
      "epoch 18: loss is 0.3063450753688812\n",
      "epoch 19: loss is 0.2957889139652252\n",
      "epoch 20: loss is 0.28633853793144226\n",
      "epoch 21: loss is 0.277883917093277\n",
      "epoch 22: loss is 0.27032747864723206\n",
      "epoch 23: loss is 0.2635810375213623\n",
      "epoch 24: loss is 0.25756365060806274\n",
      "epoch 25: loss is 0.2522004544734955\n",
      "epoch 26: loss is 0.2474217265844345\n",
      "epoch 27: loss is 0.24316245317459106\n",
      "epoch 28: loss is 0.23936223983764648\n",
      "epoch 29: loss is 0.23596537113189697\n",
      "epoch 30: loss is 0.23292088508605957\n",
      "epoch 31: loss is 0.23018254339694977\n",
      "epoch 32: loss is 0.22770877182483673\n",
      "epoch 33: loss is 0.22546283900737762\n",
      "epoch 34: loss is 0.22341233491897583\n",
      "epoch 35: loss is 0.22152909636497498\n",
      "epoch 36: loss is 0.2197887897491455\n",
      "epoch 37: loss is 0.21817076206207275\n",
      "epoch 38: loss is 0.21665732562541962\n",
      "epoch 39: loss is 0.21523353457450867\n",
      "epoch 40: loss is 0.21388697624206543\n",
      "epoch 41: loss is 0.2126072198152542\n",
      "epoch 42: loss is 0.2113855481147766\n",
      "epoch 43: loss is 0.21021471917629242\n",
      "epoch 44: loss is 0.20908872783184052\n",
      "epoch 45: loss is 0.20800261199474335\n",
      "epoch 46: loss is 0.20695222914218903\n",
      "epoch 47: loss is 0.205934077501297\n",
      "epoch 48: loss is 0.20494528114795685\n",
      "epoch 49: loss is 0.20398345589637756\n",
      "epoch 50: loss is 0.2030465006828308\n",
      "epoch 51: loss is 0.2021327167749405\n",
      "epoch 52: loss is 0.20124056935310364\n",
      "epoch 53: loss is 0.20036877691745758\n",
      "epoch 54: loss is 0.19951629638671875\n",
      "epoch 55: loss is 0.19868209958076477\n",
      "epoch 56: loss is 0.1978653222322464\n",
      "epoch 57: loss is 0.19706521928310394\n",
      "epoch 58: loss is 0.1962810903787613\n",
      "epoch 59: loss is 0.1955123245716095\n",
      "epoch 60: loss is 0.1947583705186844\n",
      "epoch 61: loss is 0.19401869177818298\n",
      "epoch 62: loss is 0.1932927668094635\n",
      "epoch 63: loss is 0.19258025288581848\n",
      "epoch 64: loss is 0.19188062846660614\n",
      "epoch 65: loss is 0.19119355082511902\n",
      "epoch 66: loss is 0.19051863253116608\n",
      "epoch 67: loss is 0.18985556066036224\n",
      "epoch 68: loss is 0.18920394778251648\n",
      "epoch 69: loss is 0.1885635256767273\n",
      "epoch 70: loss is 0.18793398141860962\n",
      "epoch 71: loss is 0.18731503188610077\n",
      "epoch 72: loss is 0.18670637905597687\n",
      "epoch 73: loss is 0.1861078143119812\n",
      "epoch 74: loss is 0.1855190098285675\n",
      "epoch 75: loss is 0.18493980169296265\n",
      "epoch 76: loss is 0.18436989188194275\n",
      "epoch 77: loss is 0.1838090568780899\n",
      "epoch 78: loss is 0.1832571178674698\n",
      "epoch 79: loss is 0.1827138513326645\n",
      "epoch 80: loss is 0.1821790486574173\n",
      "epoch 81: loss is 0.18165253102779388\n",
      "epoch 82: loss is 0.18113406002521515\n",
      "epoch 83: loss is 0.18062353134155273\n",
      "epoch 84: loss is 0.18012069165706635\n",
      "epoch 85: loss is 0.17962540686130524\n",
      "epoch 86: loss is 0.17913752794265747\n",
      "epoch 87: loss is 0.17865687608718872\n",
      "epoch 88: loss is 0.17818325757980347\n",
      "epoch 89: loss is 0.17771658301353455\n",
      "epoch 90: loss is 0.17725667357444763\n",
      "epoch 91: loss is 0.17680341005325317\n",
      "epoch 92: loss is 0.17635656893253326\n",
      "epoch 93: loss is 0.17591612040996552\n",
      "epoch 94: loss is 0.1754819005727768\n",
      "epoch 95: loss is 0.17505376040935516\n",
      "epoch 96: loss is 0.17463161051273346\n",
      "epoch 97: loss is 0.17421533167362213\n",
      "epoch 98: loss is 0.17380475997924805\n",
      "epoch 99: loss is 0.17339982092380524\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module): \n",
    "    def forward(self,x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 0.9294375777244568\n",
      "epoch 1: loss is 0.35687896609306335\n",
      "epoch 2: loss is 0.15387175977230072\n",
      "epoch 3: loss is 0.36597418785095215\n",
      "epoch 4: loss is 0.1276821792125702\n",
      "epoch 5: loss is 0.2119918167591095\n",
      "epoch 6: loss is 0.2661876678466797\n",
      "epoch 7: loss is 0.24283650517463684\n",
      "epoch 8: loss is 0.19285379350185394\n",
      "epoch 9: loss is 0.15709592401981354\n",
      "epoch 10: loss is 0.1441585272550583\n",
      "epoch 11: loss is 0.17221440374851227\n",
      "epoch 12: loss is 0.18947114050388336\n",
      "epoch 13: loss is 0.16572603583335876\n",
      "epoch 14: loss is 0.14370092749595642\n",
      "epoch 15: loss is 0.14575855433940887\n",
      "epoch 16: loss is 0.15290257334709167\n",
      "epoch 17: loss is 0.1561116725206375\n",
      "epoch 18: loss is 0.15422974526882172\n",
      "epoch 19: loss is 0.1465693563222885\n",
      "epoch 20: loss is 0.13486307859420776\n",
      "epoch 21: loss is 0.12525872886180878\n",
      "epoch 22: loss is 0.12379186600446701\n",
      "epoch 23: loss is 0.12727829813957214\n",
      "epoch 24: loss is 0.12483032047748566\n",
      "epoch 25: loss is 0.11666157841682434\n",
      "epoch 26: loss is 0.11186005175113678\n",
      "epoch 27: loss is 0.11216991394758224\n",
      "epoch 28: loss is 0.11300669610500336\n",
      "epoch 29: loss is 0.11092063784599304\n",
      "epoch 30: loss is 0.1060548946261406\n",
      "epoch 31: loss is 0.10318057239055634\n",
      "epoch 32: loss is 0.10577037185430527\n",
      "epoch 33: loss is 0.10447880625724792\n",
      "epoch 34: loss is 0.09920036792755127\n",
      "epoch 35: loss is 0.1001327782869339\n",
      "epoch 36: loss is 0.10250214487314224\n",
      "epoch 37: loss is 0.10035155713558197\n",
      "epoch 38: loss is 0.09743593633174896\n",
      "epoch 39: loss is 0.0988919734954834\n",
      "epoch 40: loss is 0.09838584065437317\n",
      "epoch 41: loss is 0.09541073441505432\n",
      "epoch 42: loss is 0.09565442055463791\n",
      "epoch 43: loss is 0.09615345299243927\n",
      "epoch 44: loss is 0.09461621195077896\n",
      "epoch 45: loss is 0.09308578073978424\n",
      "epoch 46: loss is 0.09275566786527634\n",
      "epoch 47: loss is 0.09210030734539032\n",
      "epoch 48: loss is 0.09100931882858276\n",
      "epoch 49: loss is 0.09096342325210571\n",
      "epoch 50: loss is 0.09069664776325226\n",
      "epoch 51: loss is 0.0897069126367569\n",
      "epoch 52: loss is 0.08944253623485565\n",
      "epoch 53: loss is 0.08938342332839966\n",
      "epoch 54: loss is 0.08858141303062439\n",
      "epoch 55: loss is 0.08816968649625778\n",
      "epoch 56: loss is 0.08790887147188187\n",
      "epoch 57: loss is 0.08723372220993042\n",
      "epoch 58: loss is 0.08685178309679031\n",
      "epoch 59: loss is 0.08655217289924622\n",
      "epoch 60: loss is 0.08597740530967712\n",
      "epoch 61: loss is 0.08552143722772598\n",
      "epoch 62: loss is 0.0852089375257492\n",
      "epoch 63: loss is 0.08483894169330597\n",
      "epoch 64: loss is 0.08435777574777603\n",
      "epoch 65: loss is 0.08404532819986343\n",
      "epoch 66: loss is 0.08372526615858078\n",
      "epoch 67: loss is 0.0833330899477005\n",
      "epoch 68: loss is 0.08296361565589905\n",
      "epoch 69: loss is 0.08265922218561172\n",
      "epoch 70: loss is 0.0822426900267601\n",
      "epoch 71: loss is 0.08187709003686905\n",
      "epoch 72: loss is 0.08155017346143723\n",
      "epoch 73: loss is 0.08111970126628876\n",
      "epoch 74: loss is 0.08073315024375916\n",
      "epoch 75: loss is 0.08040617406368256\n",
      "epoch 76: loss is 0.07996142655611038\n",
      "epoch 77: loss is 0.0796155035495758\n",
      "epoch 78: loss is 0.07923023402690887\n",
      "epoch 79: loss is 0.07884054630994797\n",
      "epoch 80: loss is 0.07847786694765091\n",
      "epoch 81: loss is 0.07805879414081573\n",
      "epoch 82: loss is 0.07770778983831406\n",
      "epoch 83: loss is 0.07729635387659073\n",
      "epoch 84: loss is 0.07691270112991333\n",
      "epoch 85: loss is 0.07652175426483154\n",
      "epoch 86: loss is 0.0761183574795723\n",
      "epoch 87: loss is 0.07571305334568024\n",
      "epoch 88: loss is 0.07528484612703323\n",
      "epoch 89: loss is 0.07487329840660095\n",
      "epoch 90: loss is 0.0744323879480362\n",
      "epoch 91: loss is 0.07400267571210861\n",
      "epoch 92: loss is 0.07356332987546921\n",
      "epoch 93: loss is 0.07311993837356567\n",
      "epoch 94: loss is 0.07267004251480103\n",
      "epoch 95: loss is 0.07221409678459167\n",
      "epoch 96: loss is 0.07175331562757492\n",
      "epoch 97: loss is 0.07128600776195526\n",
      "epoch 98: loss is 0.07081133127212524\n",
      "epoch 99: loss is 0.0703301951289177\n"
     ]
    }
   ],
   "source": [
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,100),\n",
    "    Swish(),\n",
    "    torch.nn.Linear(100,100),\n",
    "    Swish(),\n",
    "    torch.nn.Linear(100,3),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model accuracy: 0.9429429429429429\n",
      "mlp model accuracy: 0.975975975975976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "linear_predictions = linear_model(X).argmax(dim=1).numpy()\n",
    "mlp_predictions = mlp_model(X).argmax(dim=1).numpy()\n",
    "\n",
    "true_labels = y.numpy()\n",
    "\n",
    "linear_accuracy = accuracy_score(true_labels,linear_predictions)\n",
    "mlp_accuracy = accuracy_score(true_labels,mlp_predictions)\n",
    "\n",
    "print(f'linear model accuracy: {linear_accuracy}')\n",
    "print(f'mlp model accuracy: {mlp_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
