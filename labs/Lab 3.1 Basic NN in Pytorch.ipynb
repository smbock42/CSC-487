{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 1.1994599103927612\n",
      "epoch 1: loss is 1.1479090452194214\n",
      "epoch 2: loss is 1.0977424383163452\n",
      "epoch 3: loss is 1.048858880996704\n",
      "epoch 4: loss is 1.0012003183364868\n",
      "epoch 5: loss is 0.9547415971755981\n",
      "epoch 6: loss is 0.9094857573509216\n",
      "epoch 7: loss is 0.8654584288597107\n",
      "epoch 8: loss is 0.8227044939994812\n",
      "epoch 9: loss is 0.7812856435775757\n",
      "epoch 10: loss is 0.7412770986557007\n",
      "epoch 11: loss is 0.7027654647827148\n",
      "epoch 12: loss is 0.6658433079719543\n",
      "epoch 13: loss is 0.630605936050415\n",
      "epoch 14: loss is 0.5971452593803406\n",
      "epoch 15: loss is 0.5655445456504822\n",
      "epoch 16: loss is 0.5358721017837524\n",
      "epoch 17: loss is 0.5081758499145508\n",
      "epoch 18: loss is 0.4824797809123993\n",
      "epoch 19: loss is 0.4587804079055786\n",
      "epoch 20: loss is 0.4370463490486145\n",
      "epoch 21: loss is 0.4172193706035614\n",
      "epoch 22: loss is 0.39921584725379944\n",
      "epoch 23: loss is 0.3829289674758911\n",
      "epoch 24: loss is 0.3682323396205902\n",
      "epoch 25: loss is 0.35498398542404175\n",
      "epoch 26: loss is 0.3430333733558655\n",
      "epoch 27: loss is 0.3322295546531677\n",
      "epoch 28: loss is 0.3224288821220398\n",
      "epoch 29: loss is 0.31350094079971313\n",
      "epoch 30: loss is 0.30533120036125183\n",
      "epoch 31: loss is 0.2978217899799347\n",
      "epoch 32: loss is 0.29089030623435974\n",
      "epoch 33: loss is 0.28446754813194275\n",
      "epoch 34: loss is 0.2784958779811859\n",
      "epoch 35: loss is 0.2729266583919525\n",
      "epoch 36: loss is 0.2677188515663147\n",
      "epoch 37: loss is 0.2628374695777893\n",
      "epoch 38: loss is 0.25825265049934387\n",
      "epoch 39: loss is 0.2539381980895996\n",
      "epoch 40: loss is 0.2498714178800583\n",
      "epoch 41: loss is 0.2460324466228485\n",
      "epoch 42: loss is 0.24240341782569885\n",
      "epoch 43: loss is 0.23896853625774384\n",
      "epoch 44: loss is 0.23571369051933289\n",
      "epoch 45: loss is 0.23262599110603333\n",
      "epoch 46: loss is 0.2296939641237259\n",
      "epoch 47: loss is 0.22690697014331818\n",
      "epoch 48: loss is 0.22425544261932373\n",
      "epoch 49: loss is 0.22173066437244415\n",
      "epoch 50: loss is 0.21932446956634521\n",
      "epoch 51: loss is 0.21702945232391357\n",
      "epoch 52: loss is 0.21483877301216125\n",
      "epoch 53: loss is 0.21274608373641968\n",
      "epoch 54: loss is 0.21074557304382324\n",
      "epoch 55: loss is 0.20883171260356903\n",
      "epoch 56: loss is 0.20699957013130188\n",
      "epoch 57: loss is 0.20524434745311737\n",
      "epoch 58: loss is 0.20356175303459167\n",
      "epoch 59: loss is 0.20194768905639648\n",
      "epoch 60: loss is 0.2003983110189438\n",
      "epoch 61: loss is 0.19891013205051422\n",
      "epoch 62: loss is 0.19747984409332275\n",
      "epoch 63: loss is 0.1961042881011963\n",
      "epoch 64: loss is 0.1947806179523468\n",
      "epoch 65: loss is 0.19350607693195343\n",
      "epoch 66: loss is 0.19227813184261322\n",
      "epoch 67: loss is 0.19109436869621277\n",
      "epoch 68: loss is 0.18995258212089539\n",
      "epoch 69: loss is 0.18885062634944916\n",
      "epoch 70: loss is 0.1877865344285965\n",
      "epoch 71: loss is 0.18675844371318817\n",
      "epoch 72: loss is 0.1857646405696869\n",
      "epoch 73: loss is 0.18480342626571655\n",
      "epoch 74: loss is 0.1838732361793518\n",
      "epoch 75: loss is 0.18297266960144043\n",
      "epoch 76: loss is 0.18210028111934662\n",
      "epoch 77: loss is 0.18125484883785248\n",
      "epoch 78: loss is 0.18043506145477295\n",
      "epoch 79: loss is 0.1796398162841797\n",
      "epoch 80: loss is 0.17886801064014435\n",
      "epoch 81: loss is 0.17811860144138336\n",
      "epoch 82: loss is 0.17739063501358032\n",
      "epoch 83: loss is 0.17668312788009644\n",
      "epoch 84: loss is 0.17599527537822723\n",
      "epoch 85: loss is 0.17532622814178467\n",
      "epoch 86: loss is 0.17467516660690308\n",
      "epoch 87: loss is 0.17404142022132874\n",
      "epoch 88: loss is 0.17342425882816315\n",
      "epoch 89: loss is 0.1728229522705078\n",
      "epoch 90: loss is 0.17223696410655975\n",
      "epoch 91: loss is 0.17166562378406525\n",
      "epoch 92: loss is 0.1711084097623825\n",
      "epoch 93: loss is 0.17056475579738617\n",
      "epoch 94: loss is 0.17003415524959564\n",
      "epoch 95: loss is 0.16951610147953033\n",
      "epoch 96: loss is 0.16901017725467682\n",
      "epoch 97: loss is 0.1685158759355545\n",
      "epoch 98: loss is 0.16803282499313354\n",
      "epoch 99: loss is 0.1675606071949005\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module): \n",
    "    def forward(self,x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 1.344245433807373\n",
      "epoch 1: loss is 0.6275767087936401\n",
      "epoch 2: loss is 0.5275899171829224\n",
      "epoch 3: loss is 0.47496917843818665\n",
      "epoch 4: loss is 0.43775811791419983\n",
      "epoch 5: loss is 0.4086676239967346\n",
      "epoch 6: loss is 0.38483893871307373\n",
      "epoch 7: loss is 0.364777147769928\n",
      "epoch 8: loss is 0.34756264090538025\n",
      "epoch 9: loss is 0.3325784504413605\n",
      "epoch 10: loss is 0.319388210773468\n",
      "epoch 11: loss is 0.3076719343662262\n",
      "epoch 12: loss is 0.29718729853630066\n",
      "epoch 13: loss is 0.28774598240852356\n",
      "epoch 14: loss is 0.27919870615005493\n",
      "epoch 15: loss is 0.27142465114593506\n",
      "epoch 16: loss is 0.26432475447654724\n",
      "epoch 17: loss is 0.25781649351119995\n",
      "epoch 18: loss is 0.2518306374549866\n",
      "epoch 19: loss is 0.24630843102931976\n",
      "epoch 20: loss is 0.24119971692562103\n",
      "epoch 21: loss is 0.23646116256713867\n",
      "epoch 22: loss is 0.23205533623695374\n",
      "epoch 23: loss is 0.22794964909553528\n",
      "epoch 24: loss is 0.2241155058145523\n",
      "epoch 25: loss is 0.220527783036232\n",
      "epoch 26: loss is 0.21716445684432983\n",
      "epoch 27: loss is 0.21400578320026398\n",
      "epoch 28: loss is 0.21103429794311523\n",
      "epoch 29: loss is 0.2082344889640808\n",
      "epoch 30: loss is 0.20559240877628326\n",
      "epoch 31: loss is 0.20309558510780334\n",
      "epoch 32: loss is 0.20073270797729492\n",
      "epoch 33: loss is 0.19849365949630737\n",
      "epoch 34: loss is 0.19636929035186768\n",
      "epoch 35: loss is 0.19435131549835205\n",
      "epoch 36: loss is 0.19243212044239044\n",
      "epoch 37: loss is 0.19060486555099487\n",
      "epoch 38: loss is 0.18886329233646393\n",
      "epoch 39: loss is 0.18720164895057678\n",
      "epoch 40: loss is 0.18561474978923798\n",
      "epoch 41: loss is 0.18409770727157593\n",
      "epoch 42: loss is 0.1826462298631668\n",
      "epoch 43: loss is 0.18125614523887634\n",
      "epoch 44: loss is 0.17992378771305084\n",
      "epoch 45: loss is 0.17864567041397095\n",
      "epoch 46: loss is 0.17741860449314117\n",
      "epoch 47: loss is 0.17623968422412872\n",
      "epoch 48: loss is 0.17510613799095154\n",
      "epoch 49: loss is 0.17401540279388428\n",
      "epoch 50: loss is 0.1729651838541031\n",
      "epoch 51: loss is 0.17195329070091248\n",
      "epoch 52: loss is 0.1709776222705841\n",
      "epoch 53: loss is 0.17003630101680756\n",
      "epoch 54: loss is 0.16912758350372314\n",
      "epoch 55: loss is 0.16824978590011597\n",
      "epoch 56: loss is 0.16740138828754425\n",
      "epoch 57: loss is 0.16658088564872742\n",
      "epoch 58: loss is 0.1657869666814804\n",
      "epoch 59: loss is 0.16501833498477936\n",
      "epoch 60: loss is 0.16427378356456757\n",
      "epoch 61: loss is 0.1635522097349167\n",
      "epoch 62: loss is 0.16285257041454315\n",
      "epoch 63: loss is 0.162173792719841\n",
      "epoch 64: loss is 0.1615150272846222\n",
      "epoch 65: loss is 0.1608753502368927\n",
      "epoch 66: loss is 0.16025391221046448\n",
      "epoch 67: loss is 0.15964993834495544\n",
      "epoch 68: loss is 0.1590627282857895\n",
      "epoch 69: loss is 0.15849150717258453\n",
      "epoch 70: loss is 0.15793567895889282\n",
      "epoch 71: loss is 0.15739452838897705\n",
      "epoch 72: loss is 0.15686754882335663\n",
      "epoch 73: loss is 0.1563541144132614\n",
      "epoch 74: loss is 0.15585365891456604\n",
      "epoch 75: loss is 0.15536576509475708\n",
      "epoch 76: loss is 0.15488991141319275\n",
      "epoch 77: loss is 0.15442562103271484\n",
      "epoch 78: loss is 0.15397244691848755\n",
      "epoch 79: loss is 0.15353000164031982\n",
      "epoch 80: loss is 0.15309785306453705\n",
      "epoch 81: loss is 0.15267565846443176\n",
      "epoch 82: loss is 0.15226304531097412\n",
      "epoch 83: loss is 0.15185967087745667\n",
      "epoch 84: loss is 0.15146517753601074\n",
      "epoch 85: loss is 0.15107932686805725\n",
      "epoch 86: loss is 0.15070173144340515\n",
      "epoch 87: loss is 0.15033213794231415\n",
      "epoch 88: loss is 0.14997033774852753\n",
      "epoch 89: loss is 0.14961597323417664\n",
      "epoch 90: loss is 0.14926888048648834\n",
      "epoch 91: loss is 0.14892873167991638\n",
      "epoch 92: loss is 0.14859536290168762\n",
      "epoch 93: loss is 0.14826853573322296\n",
      "epoch 94: loss is 0.14794804155826569\n",
      "epoch 95: loss is 0.1476336568593979\n",
      "epoch 96: loss is 0.14732526242733002\n",
      "epoch 97: loss is 0.14702261984348297\n",
      "epoch 98: loss is 0.14672552049160004\n",
      "epoch 99: loss is 0.14643383026123047\n"
     ]
    }
   ],
   "source": [
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,100),\n",
    "    Swish(),\n",
    "    torch.nn.Linear(100,100),\n",
    "    Swish(),\n",
    "    torch.nn.Linear(100,3),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(mlp_model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model accuracy: 0.948948948948949\n",
      "mlp model accuracy: 0.948948948948949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "linear_predictions = linear_model(X).argmax(dim=1).numpy()\n",
    "mlp_predictions = mlp_model(X).argmax(dim=1).numpy()\n",
    "\n",
    "true_labels = y.numpy()\n",
    "\n",
    "linear_accuracy = accuracy_score(true_labels,linear_predictions)\n",
    "mlp_accuracy = accuracy_score(true_labels,mlp_predictions)\n",
    "\n",
    "print(f'linear model accuracy: {linear_accuracy}')\n",
    "print(f'mlp model accuracy: {mlp_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
